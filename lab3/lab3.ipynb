{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **ЛР №3**: Формирование данных для машинного обучения в формате NumPy на основе датасета Semantic3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Ход работы**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RsOtV_41hytv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Первые 5 строк массива:\n",
            "[[-0.07775801 -2.07133174 -0.5946238  -0.52549022  0.50980395  0.57254905\n",
            "   0.63529414  0.        ]\n",
            " [-0.07759166 -2.07160068 -0.65121341 -0.34901962  0.55686277  0.6156863\n",
            "   0.69803923  0.        ]\n",
            " [-0.07767483 -2.07180238 -0.67875969 -1.10196078  0.56470591  0.6156863\n",
            "   0.6901961   0.        ]\n",
            " [-0.07767483 -2.07180238 -0.67965794 -0.63529414  0.56078434  0.61176473\n",
            "   0.68627453  0.        ]\n",
            " [-0.07775801 -2.07180238 -0.67965794 -0.44313726  0.56078434  0.61176473\n",
            "   0.68627453  0.        ]]\n",
            "\n",
            "Форма итогового массива: (23995481, 8)\n",
            "Тип данных: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def process_semantic3d_data(points_file, labels_file, output_file='semantic3d_dataset.npy'):\n",
        "    try:\n",
        "        with open(points_file, 'r') as f:\n",
        "            first_line = f.readline().strip().split()\n",
        "            num_columns = len(first_line)\n",
        "        \n",
        "        if num_columns == 7:\n",
        "            data = np.genfromtxt(points_file, dtype=np.float32, comments=None, autostrip=True)\n",
        "            x = data[:, 0]\n",
        "            y = data[:, 1]\n",
        "            z = data[:, 2]\n",
        "            r = data[:, 3]\n",
        "            g = data[:, 4]\n",
        "            b = data[:, 5]\n",
        "            intensity = data[:, 6]\n",
        "        elif num_columns == 6:\n",
        "            data = np.genfromtxt(points_file, dtype=np.float32, comments=None, autostrip=True)\n",
        "            x = data[:, 0]\n",
        "            y = data[:, 1]\n",
        "            z = data[:, 2]\n",
        "            r = data[:, 3]\n",
        "            g = data[:, 4]\n",
        "            b = data[:, 5]\n",
        "            intensity = np.zeros(len(data), dtype=np.float32)\n",
        "        elif num_columns == 4:\n",
        "            data = np.genfromtxt(points_file, dtype=np.float32, comments=None, autostrip=True)\n",
        "            x = data[:, 0]\n",
        "            y = data[:, 1]\n",
        "            z = data[:, 2]\n",
        "            intensity = data[:, 3]\n",
        "            r = g = b = np.zeros(len(data), dtype=np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Неподдерживаемый формат файла с {num_columns} столбцами\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке данных: {e}\")\n",
        "        print(\"Попытка ручной загрузки данных...\")\n",
        "        x, y, z, r, g, b, intensity = [], [], [], [], [], [], []\n",
        "        \n",
        "        with open(points_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) not in (4, 6, 7):\n",
        "                    continue\n",
        "                \n",
        "                vals = list(map(float, parts))     \n",
        "                if len(vals) == 7:\n",
        "                    x.append(vals[0])\n",
        "                    y.append(vals[1])\n",
        "                    z.append(vals[2])\n",
        "                    r.append(vals[3])\n",
        "                    g.append(vals[4])\n",
        "                    b.append(vals[5])\n",
        "                    intensity.append(vals[6])\n",
        "                elif len(vals) == 6:\n",
        "                    x.append(vals[0])\n",
        "                    y.append(vals[1])\n",
        "                    z.append(vals[2])\n",
        "                    r.append(vals[3])\n",
        "                    g.append(vals[4])\n",
        "                    b.append(vals[5])\n",
        "                    intensity.append(0.0)\n",
        "                elif len(vals) == 4:\n",
        "                    x.append(vals[0])\n",
        "                    y.append(vals[1])\n",
        "                    z.append(vals[2])\n",
        "                    intensity.append(vals[3])\n",
        "                    r.append(0.0)\n",
        "                    g.append(0.0)\n",
        "                    b.append(0.0)\n",
        "        \n",
        "        x = np.array(x, dtype=np.float32)\n",
        "        y = np.array(y, dtype=np.float32)\n",
        "        z = np.array(z, dtype=np.float32)\n",
        "        r = np.array(r, dtype=np.float32)\n",
        "        g = np.array(g, dtype=np.float32)\n",
        "        b = np.array(b, dtype=np.float32)\n",
        "        intensity = np.array(intensity, dtype=np.float32)\n",
        "\n",
        "    labels = np.loadtxt(labels_file, dtype=np.int32)\n",
        "    \n",
        "    min_len = min(len(x), len(labels))\n",
        "    x, y, z = x[:min_len], y[:min_len], z[:min_len]\n",
        "    r, g, b = r[:min_len], g[:min_len], b[:min_len]\n",
        "    intensity = intensity[:min_len]\n",
        "    labels = labels[:min_len]\n",
        "    \n",
        "    r_norm = r / 255.0\n",
        "    g_norm = g / 255.0\n",
        "    b_norm = b / 255.0\n",
        "    \n",
        "    x_mean, x_std = np.mean(x), np.std(x)\n",
        "    y_mean, y_std = np.mean(y), np.std(y)\n",
        "    z_mean, z_std = np.mean(z), np.std(z)\n",
        "    \n",
        "    if x_std > 0: x_norm = (x - x_mean) / x_std\n",
        "    else: x_norm = x - x_mean\n",
        "    \n",
        "    if y_std > 0: y_norm = (y - y_mean) / y_std\n",
        "    else: y_norm = y - y_mean\n",
        "    \n",
        "    if z_std > 0: z_norm = (z - z_mean) / z_std\n",
        "    else: z_norm = z - z_mean\n",
        "    \n",
        "    intensity_min, intensity_max = np.min(intensity), np.max(intensity)\n",
        "    if intensity_max > intensity_min:\n",
        "        intensity_norm = (intensity - intensity_min) / (intensity_max - intensity_min)\n",
        "    else:\n",
        "        intensity_norm = intensity\n",
        "    \n",
        "    features = np.column_stack((\n",
        "        x_norm, y_norm, z_norm,\n",
        "        r_norm, g_norm, b_norm,\n",
        "        intensity_norm\n",
        "    ))\n",
        "    \n",
        "    labels_reshaped = labels.reshape(-1, 1)\n",
        "    dataset = np.hstack((features, labels_reshaped))\n",
        "    \n",
        "    np.save(output_file, dataset)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    unique_classes, counts = np.unique(labels, return_counts=True)\n",
        "    plt.bar(unique_classes, counts)\n",
        "    plt.xlabel('Класс')\n",
        "    plt.ylabel('Количество точек')\n",
        "    plt.title('Распределение меток по классам')\n",
        "    plt.xticks(unique_classes)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('class_distribution.png')\n",
        "    plt.close()\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "dataset = process_semantic3d_data(\n",
        "    \"bildstein_station3_xyz_intensity_rgb.txt\",\n",
        "    \"bildstein_station3_xyz_intensity_rgb.labels\"\n",
        ")\n",
        "\n",
        "print(\"Первые 5 строк массива:\")\n",
        "print(dataset[:5])\n",
        "print(\"\\nФорма итогового массива:\", dataset.shape)\n",
        "print(\"Тип данных:\", dataset.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ответы на контрольные вопросы\n",
        "\n",
        "1. **Дайте определение датасету Semantic3D. Каковы его основные характеристики и для каких задач он предназначен?**\n",
        "Semantic3D — это датасет для семантической сегментации 3D облаков точек, полученных с помощью наземного лазерного сканирования (LiDAR). Содержит миллиарды точек outdoor-сцен (улицы, здания, парки) с 8 классами объектов. Предназначен для обучения моделей 3D-сегментации, распознавания объектов в автономных системах и роботах.\n",
        "\n",
        "2. **Чем Semantic3D принципиально отличается от датасетов для 2D-компьютерного зрения и других 3D-датасетов для помещений?**\n",
        "Отличается от 2D-датасетов (ImageNet) тем, что работает с 3D-координатами вместо пикселей. От indoor-3D датасетов (S3DIS) отличается тем, что содержит outdoor-сцены с большими площадями, имеет неравномерную плотность точек из-за особенностей наземного сканирования и включает другие объекты (дороги, деревья, автомобили).\n",
        "\n",
        "3. **Опишите состав и структуру датасета. На какие множества он разделен?**\n",
        "Датасет состоит из 30 облаков точек, разделенных на обучающее (15 сцен) и тестовое (15 сцен) множества. Каждая сцена представлена файлом .txt с координатами, интенсивностью и цветами точек, и файлом .labels с метками классов для каждой точки.\n",
        "\n",
        "4. **Каким способом были получены данные в Semantic3D?**\n",
        "Данные получены с помощью наземного лазерного сканирования (LiDAR), где сканеры устанавливались на неподвижные позиции или на автомобили. Это приводит к неравномерной плотности точек (высокая вблизи сканера, низкая на расстоянии) и наличию шума от отражений и ошибок измерений.\n",
        "\n",
        "5. **Перечислите и охарактеризуйте 8 семантических классов в датасете.**\n",
        "    1. Мост (bridge) - пешеходные и автомобильные мосты\n",
        "    2. Здание (building) - жилые и коммерческие здания\n",
        "    3. Дорога (road) - проезжие части, тротуары\n",
        "    4. Столбы (pole) - фонарные столбы, дорожные знаки\n",
        "    5. Земля (ground) - почва, трава\n",
        "    6. Растения (vegetation) - деревья, кусты\n",
        "    7. Пешеход (human) - люди на улицах\n",
        "    8. Автомобиль (car) - легковые и грузовые автомобили\n",
        "\n",
        "6. **Каковы основные проблемы, связанные с разметкой данных?**\n",
        "Основные проблемы: субъективность при определении границ объектов, наличие шума и артефактов сканирования, высокая трудоемкость ручной разметки миллиардов точек, несогласованность разметки между разными аннотаторами, сложности с разметкой точек на границах объектов.\n",
        "\n",
        "7. **Назовите основные технические проблемы при работе с облаками точек такого объема.**\n",
        "Огромные требования к памяти (RAM и GPU), медленная обработка из-за объема данных, сложности визуализации, необходимость специальных структур данных, проблемы с хранением и передачей данных.\n",
        "\n",
        "8. **Что такое \"неравномерная плотность\" точек в контексте LiDAR-данных?**\n",
        "Неравномерная плотность — это изменение количества точек на единицу площади в зависимости от расстояния до сканера. Приводит к тому, что модели лучше обучаются на объектах с высокой плотностью точек и хуже — на объектах с низкой плотностью.\n",
        "\n",
        "9. **Опишите проблему несбалансированности классов в Semantic3D.**\n",
        "Некоторые классы (земля, здания, растения) представлены тысячами точек, а другие (люди, столбы) — единицами или десятками. Это приводит к тому, что модели склонны предсказывать частые классы, игнорируя редкие, а метрика общей точности становится обманчиво высокой.\n",
        "\n",
        "10. **Какие основные этапы предобработки данных необходимы?**\n",
        "Основные этапы: нормализация координат (центрирование и масштабирование), нормализация признаков (интенсивность, RGB), даунсэмплинг для уменьшения объема данных, удаление шума, балансировка классов и аугментация данных (вращение, сдвиг)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.11.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
